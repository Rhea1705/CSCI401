Abstract
In this work we investigate the effect of the convolutional network depth on its
accuracy in the large-scale image recognition setting. Our main contribution is a
thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the
depth to 16–19 weight layers. These findings were the basis of our ImageNet
Challenge 2014 submission, where our team secured the first and the second
places in the localisation and classification tracks respectively.
1 Introduction
Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale visual
recognition [10, 16, 17, 19] which has become possible due to the large public image repositories,
such as ImageNet [4], and high-performance computing systems, such as GPUs or large-scale distributed clusters [3]. In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) [1],
which has served as a testbed for a few generations of large-scale image classification systems,
from high-dimensional shallow feature encodings [13] (the winner of ILSVRC-2011) to deep ConvNets [10] (the winner of ILSVRC-2012).
With ConvNets becoming more of a commodity in the computer vision field, a number of attempts
have been made to improve the original architecture of [10] in a bid to achieve better accuracy. For
instance, the best-performing submissions to the ILSVRC-2013 [16, 19] utilised smaller receptive
window size and smaller stride of the first convolutional layer. Another line of improvements dealt
with training and testing the networks densely over the whole image and over multiple scales [7, 16].
In this paper, we address another important aspect of ConvNet architecture design – its depth. To
this end, we fix other parameters of the architecture, and steadily increase the depth of the network
by adding more convolutional layers.
The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations.
The details of the image classification training and evaluation are then presented in Sect. 3, and the
configurations are compared on the ILSVRC classification task in Sect. 4. For completeness, we
also describe and assess our object localisation system in Sect. 5, and Sect. 6 concludes.